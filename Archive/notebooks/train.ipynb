{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lumira Tiny LLM - Training Notebook\n",
    "\n",
    "Google Colab用のLumira Transformer訓練ノートブック\n",
    "\n",
    "## セットアップ\n",
    "1. ランタイム → ランタイムのタイプを変更 → GPU (T4)\n",
    "2. 上から順にセルを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveをマウント（チェックポイント保存用）\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# リポジトリをクローン（クリーンアップしてから）\n%cd /content\n!rm -rf Lumira\n!git clone https://github.com/iwamaki/Lumira.git\n%cd /content/Lumira\n\n# 確認\n!pwd\n!ls"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存関係をインストール\n",
    "!pip install -q torch sentencepiece tokenizers tqdm pandas gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU確認\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: データ生成\n",
    "\n",
    "語彙を拡張し、訓練データを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ生成\n",
    "!python scripts/generate_data.py \\\n",
    "    --vocab-size 500 \\\n",
    "    --data-size 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成されたデータを確認\n",
    "!wc -l data/processed/train.jsonl data/processed/val.jsonl\n",
    "!head -5 data/processed/train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: トークナイザー訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザーを訓練\n",
    "!python scripts/train_tokenizer.py \\\n",
    "    --vocab-size 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: モデル訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 訓練設定\nimport sys\nsys.path.insert(0, '.')\n\nfrom src.model import ModelConfig, TINY_CONFIG\n\n# モデルサイズ確認\nconfig = TINY_CONFIG\nprint(f\"Model config: {config}\")\nprint(f\"Estimated parameters: {config.estimate_params():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 訓練開始\nfrom src.training import Trainer, TrainingConfig\n\nconfig = TrainingConfig(\n    train_data=\"data/processed/train.jsonl\",\n    val_data=\"data/processed/val.jsonl\",\n    tokenizer_model=\"data/vocab/lumira.model\",\n    output_dir=\"/content/drive/MyDrive/lumira_checkpoints\",  # Google Driveに保存\n    model_config=\"tiny\",\n    epochs=10,\n    batch_size=24,  # T4 GPU向け\n    gradient_accumulation_steps=4,\n    learning_rate=1e-4,\n    use_amp=True,\n    save_every=500,\n    eval_every=250,\n)\n\ntrainer = Trainer(config)\ntrainer.train()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 訓練再開（セッション切断後）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 訓練を再開する場合\nfrom src.training import Trainer, TrainingConfig\n\nconfig = TrainingConfig(\n    train_data=\"data/processed/train.jsonl\",\n    val_data=\"data/processed/val.jsonl\",\n    tokenizer_model=\"data/vocab/lumira.model\",\n    output_dir=\"/content/drive/MyDrive/lumira_checkpoints\",\n    model_config=\"tiny\",\n    epochs=10,\n    batch_size=24,\n    gradient_accumulation_steps=4,\n    use_amp=True,\n    resume_from=\"/content/drive/MyDrive/lumira_checkpoints/step_1000.pt\",  # チェックポイントを指定\n)\n\ntrainer = Trainer(config)\ntrainer.train()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 推論テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.inference import Translator\nfrom src.model import TINY_CONFIG\n\ntranslator = Translator(\n    model_path=\"/content/drive/MyDrive/lumira_checkpoints/best.pt\",\n    tokenizer_path=\"data/vocab/lumira.model\",\n    model_config=TINY_CONFIG,  # 訓練時と同じ設定を使用\n)\n\n# テスト翻訳\ntest_sentences = [\n    \"こんにちは\",\n    \"ありがとう\",\n    \"私はあなたを愛しています\",\n    \"今日は美しい日です\",\n    \"私たちは成長している\",\n]\n\nfor sent in test_sentences:\n    result = translator.translate(sent)\n    print(f\"{sent} → {result}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Gradio デモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference.translate import create_demo_ui\n",
    "\n",
    "demo = create_demo_ui(translator)\n",
    "demo.launch(share=True)  # share=Trueで公開URLを生成"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}